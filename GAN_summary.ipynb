{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_summary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmrqiG/oNepdb2inisNfIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aakanksha1111/ClusterAnalysisHerwig/blob/master/GAN_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H4NAvhUAMg5"
      },
      "source": [
        "# Loading and picking up our training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSufOmZBRQym"
      },
      "source": [
        "First, we import all the packages and libraries needed to run the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp_e1YTxFJj2"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from numpy import hstack\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import rand\n",
        "from numpy.random import randn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-SazhQRRjW5"
      },
      "source": [
        "Two Methods of uploading data to google colab. \\\\\n",
        "Method 1 : Gives Public access, without access of any drive. Right click the file and cLick at get link option, then change the privacy of data file to \"anyone with the link can view\". Copy the document id. \\\\\n",
        "Method 2 : Requires a drive access, one needs to upload all files in the drive and then mount the drive here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CihKgkVaShZb"
      },
      "source": [
        "#Method 1 : Public access\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=FILE_ID' -O data2\n",
        "data=np.load('/content/data2')\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8shNsF1WSwdf"
      },
      "source": [
        "#Method 2 :\n",
        "!pip install PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#2. Get the file\n",
        "downloaded = drive.CreateFile({'id':\"FILE_ID\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('data.npy')\n",
        "\n",
        "data = np.load('data.npy', encoding='bytes')\n",
        "data\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDqq9kmIS-67"
      },
      "source": [
        "Now we want that each row contains varaibles coressponding to same decay event i.e. Cluster decaying to two decay products. So,we reshape the data by clubbing three rows into one. Thus, no of rows reduce to 1/3 of previous value and no of columns become 3 times. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEcBMoJKTsjA"
      },
      "source": [
        "data=data.reshape((int(data.shape[0]/3),int(data.shape[1]*3)))\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJK_hmWyUocb"
      },
      "source": [
        "Here are the details of how data is stored after reshaping :\n",
        "*   It is now 2D numpy array\n",
        "*   Number of rows = 1/3 of original value\n",
        "*   Number of columns = 24 (8*3)\n",
        "*   Index are 0,1, 2,.....,23\n",
        "*   Every row has variables correspoinding to same decay event.\n",
        "*   Indexing of columns is as follows : \\\\\n",
        "Cluster - 0 to 7, Particle1 - 8 to 15, Particle2 - 16 to 23\n",
        "\n",
        "| Main Event Number | PDG ID| Energy | Px | Py |Pz | Invariant Mass | On Shell Condition Value|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWEWYHX5ZUcj"
      },
      "source": [
        "Now we weite a code which will extract those decay events which has pi+, pi- or pi0 whose PDG IDs are 211 , -211 , 111 respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pQBI1zZUm7b"
      },
      "source": [
        "# data.shape[0] = 275595\n",
        "i=0 \n",
        "k=0\n",
        "data_pi = zeros((1,24))                # Create a temporary row of zerores\n",
        "while i < data.shape[0]:               # Go through each row of data\n",
        "  set=[9,17]  # Index of particle IDs\n",
        "  flag=0   # counter\n",
        "  for j in set:\n",
        "    if (data[i][j]==111):          # Increase the counter if particle is pi+ || pi- || pi0\n",
        "      flag=flag+1\n",
        "  if flag==2 :                                  # Separate the events where both particles are pions, counter should be 2\n",
        "    t = data[i].reshape((1,24))\n",
        "    data_pi = np.concatenate((data_pi,t))       # Concatenate\n",
        "  i=i+1\n",
        "data_pi = np.delete(data_pi,0,axis=0)        # Delete the first temporary row of zerores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEO3d2vyZ8Gm"
      },
      "source": [
        "Now we have managed to extract such events which have pi+, pi- or pi0 only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruHdWEdBZ5kL"
      },
      "source": [
        "data_pi.shape[0]           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOESI8AfaL_P"
      },
      "source": [
        "Now we set up three counters and examine the various kinds of decay process combinations which are possible or not : \\\\\n",
        "(Note that, the PDG IDs for decay products are at index number 9 and 17)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61RxBoVarCmv"
      },
      "source": [
        "i=0\n",
        "k=0\n",
        "data_pi_2 = zeros((1,24))             # Create a temporary row of zerores\n",
        "data_pi_3 = zeros((1,24))             # Create a temporary row of zerores\n",
        "data_pi_4 = zeros((1,24))             # Create a temporary row of zerores\n",
        "data_pi_5 = zeros((1,24))             # Create a temporary row of zerores\n",
        "data_pi_6 = zeros((1,24))             # Create a temporary row of zerores\n",
        "data_pi_7 = zeros((1,24))             # Create a temporary row of zerores\n",
        "while i < data_pi.shape[0]:\n",
        "  set=[9,17]\n",
        "  c1=0 # counter for pi+\n",
        "  c2=0 # counter for pi-\n",
        "  c3=0 # counter for pi0\n",
        "  for j in set:\n",
        "    if (data_pi[i][j]==211):      # increment c1 if it is pi+\n",
        "      c1=c1+1\n",
        "    if (data_pi[i][j]==-211):       # increment c2 if it is pi-\n",
        "      c2=c2+1\n",
        "    if (data_pi[i][j]==111):     # increment c3 if it is pi0\n",
        "      c3=c3+1  \n",
        "\n",
        "  if (c1==1 and c2==1):                              # Separate the decays with  pi+ and pi-\n",
        "    t = data_pi[i].reshape((1,24))\n",
        "    data_pi_2 = np.concatenate((data_pi_2,t))        \n",
        "  elif (c3==2):                                      # Separate the decays with  2 pi0\n",
        "    t = data_pi[i].reshape((1,24))\n",
        "    data_pi_3 = np.concatenate((data_pi_3,t))        \n",
        "  elif (c2==2):                                      # Separate the decays with  2 pi-\n",
        "    t = data_pi[i].reshape((1,24))\n",
        "    data_pi_4 = np.concatenate((data_pi_4,t))        \n",
        "  elif (c1==2):                                      # Separate the decays with  2 pi+\n",
        "    t = data_pi[i].reshape((1,24))\n",
        "    data_pi_5 = np.concatenate((data_pi_5,t))        \n",
        "  elif (c2==1 and c3==1):                            # Separate the decays with  pi0 and pi-\n",
        "    t = data_pi[i].reshape((1,24))\n",
        "    data_pi_6 = np.concatenate((data_pi_6,t))        \n",
        "  elif (c3==1 and c1==1):                            # Separate the decays with  pi0 and pi+\n",
        "    t = data_pi[i].reshape((1,24))\n",
        "    data_pi_7 = np.concatenate((data_pi_7,t))        \n",
        "  else:\n",
        "    only\n",
        "    k=k+1\n",
        "       \n",
        "  i=i+1\n",
        "data_pi_2 = np.delete(data_pi_2,0,axis=0)         # Delete the first temporary rows of zerores\n",
        "data_pi_3 = np.delete(data_pi_3,0,axis=0)         # Delete the first temporary rows of zerores\n",
        "data_pi_4 = np.delete(data_pi_4,0,axis=0)         # Delete the first temporary rows of zerores\n",
        "data_pi_5 = np.delete(data_pi_5,0,axis=0)         # Delete the first temporary rows of zerores  \n",
        "data_pi_6 = np.delete(data_pi_6,0,axis=0)         # Delete the first temporary rows of zerores \n",
        "data_pi_7 = np.delete(data_pi_7,0,axis=0)         # Delete the first temporary rows of zerores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7v6LI-3ampu"
      },
      "source": [
        "print(k)\n",
        "print(data_pi_2.shape)\n",
        "print(data_pi_3.shape)\n",
        "print(data_pi_4.shape)\n",
        "print(data_pi_5.shape)\n",
        "print(data_pi_6.shape)\n",
        "print(data_pi_7.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y32s-ndRarPe"
      },
      "source": [
        "In the previous cell, let us examine what is the correspondance of different values : \\\\\n",
        "data_pi_2 :  Cluster -> pi+ & pi- \\\\\n",
        "data_pi_3 :  Cluster -> pi0 & pi0 \\\\\n",
        "data_pi_4 :  Cluster -> pi- & pi-  (Never Happens) \\\\\n",
        "data_pi_5 :  Cluster -> pi+ & pi+  (Never Happens) \\\\\n",
        "data_pi_6 :  Cluster -> pi0 & pi- \\\\\n",
        "data_pi_7 :  Cluster -> pi0 & pi+  \\\\\n",
        "\n",
        "\n",
        "Approach 1 : \\\\\n",
        "We will concern ourselves only with pi+ & pi- decays, then data_pi_2 will be used everywhere further and mass = 139.57 MeV (for both pi+ and pi-) will be used. \\\\\n",
        "Approach 2 : \\\\\n",
        "We will concern ourselves with cluster decay decaying to pi0 & pi0, then data_pi_3 will be used everywhere further and mass = 134.98 MeV will be used. \\\\\n",
        "\n",
        "lets, pick approach 1 this time. \\\\\n",
        "First of all, we will import the data of decay of both pi0 into our temporary data that we will work on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuCROWm9anpv"
      },
      "source": [
        "tempdata = data_pi_3 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBX777j6eem6"
      },
      "source": [
        "m = 134.98"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8HvxGBbAgv1"
      },
      "source": [
        "# Here we begin with GAN implementation :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MIDLvWbek_c"
      },
      "source": [
        "Following cell block generates n real samples, that is, it randomly extracts n rows from the temporary data we work on. It will extract specific variables only that we consider for analysis. \\\\\n",
        "px , py, pz  of Cluster and a Particle are extracted, thus shape of sample will be (n,6). \\\\\n",
        "Indexing as follows : \\\\\n",
        "0,1,2 = px, py ,pz of Cluster \\\\\n",
        "3,4,5 = px, py ,pz of any one particle \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql2ZvLqafNRI"
      },
      "source": [
        "def generate_real_samples(n):\n",
        "  i=0\n",
        "  X1 = zeros((n,3))\n",
        "  X2 = zeros((n,3))\n",
        "  b=tempdata[np.random.choice(tempdata.shape[0], n, replace=True), :]\n",
        "  while i < (n):\n",
        "    set=[3,4,5] # index for px,py,pz of cluster\n",
        "    for j in set:\n",
        "      X1[i][j-3]=b[i][j] # store those values in sample\n",
        "      \n",
        "    set=[11,12,13] # index for px,py,pz of Particle (any one)\n",
        "    for j in set:\n",
        "      X2[i][j-11]=b[i][j] # store those values in sample\n",
        "    \n",
        "    i=i+1\n",
        "  X = hstack((X1, X2))\n",
        "  y = ones((n, 1)) \n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52guyjiX1F_X"
      },
      "source": [
        "Following cell block Plots the histogram for Squared Trans. Momentum Distribution, for both Cluster & Particle # Cluster has color red and Particle has blue. It takes as input the sample generated only, ( sample can be real or fake)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-imNae7rwnJ"
      },
      "source": [
        "def display(X,Y):\n",
        "  v=zeros((X.shape[0],2))\n",
        "  i=0\n",
        "  while i < X.shape[0]:\n",
        "    v[i][0]= (X[i][0]*X[i][0])+(X[i][1]*X[i][1]) # px*px + py*py for Cluster\n",
        "    v[i][1]= (X[i][3]*X[i][3])+(X[i][4]*X[i][4]) # px*px + py*py for Particle\n",
        "    i=i+1\n",
        "  #plt.subplot(1, 2, 1)\n",
        "  #plt.hist(v[:,0],color='red',alpha=0.5,bins=np.linspace(0,1000000000*0.2*0.25*0.2,50)) # Red For Cluster\n",
        "  #plt.hist(v[:,1],color='blue',alpha=0.5,bins=np.linspace(0,1000000000*0.2*0.25*0.2,50))  # Blue for Particle\n",
        "  #plt.title('Squared pT Distribution of Real samples')\n",
        " \n",
        "\n",
        "  w=zeros((Y.shape[0],2))\n",
        "  i=0\n",
        "  while i < Y.shape[0]:\n",
        "    w[i][0]= (Y[i][0]*Y[i][0])+(Y[i][1]*Y[i][1]) # px*px + py*py for Cluster\n",
        "    w[i][1]= (Y[i][3]*Y[i][3])+(Y[i][4]*Y[i][4]) # px*px + py*py for Particle\n",
        "    i=i+1\n",
        "  #plt.subplot(1, 2, 2)\n",
        "  #plt.hist(w[:,0],color='red',alpha=0.5,bins=np.linspace(0,1000000000*0.2*0.25*0.2,50)) # Red For Cluster\n",
        "  #plt.hist(w[:,1],color='blue',alpha=0.5,bins=np.linspace(0,1000000000*0.2*0.25*0.2,50))  # Blue for Particle\n",
        "  #plt.title('Squared pT Distribution of Fake samples')\n",
        "\n",
        "  g=zeros((X.shape[0],1))          # For Real Samples\n",
        "  i=0\n",
        "  while i < X.shape[0]:\n",
        "    g[i][0]= calcmsqr(X[i][0],X[i][1],X[i][2],X[i][3],X[i][4],X[i][5],134.98,134.98)  # Calc Sq. Mass of cluster\n",
        "    i=i+1\n",
        "  \n",
        "  h=zeros((Y.shape[0],1))         # For Fake Samples\n",
        "  i=0\n",
        "  while i < Y.shape[0]:\n",
        "    h[i][0]= calcmsqr(Y[i][0],Y[i][1],Y[i][2],Y[i][3],Y[i][4],Y[i][5],134.98,134.98)  # Calc Sq. Mass of cluster\n",
        "    i=i+1\n",
        "  \n",
        " \n",
        "\n",
        "  fig, ax = plt.subplots(nrows=2,ncols= 2,figsize=(20, 10),sharex=True, sharey=True) \n",
        "  ax[0,0].hist(v[:,0],color='red',alpha=0.5,bins=np.linspace(0,1000000000*0.2*0.25*0.2,50),label='Cluster') # Red For Cluster\n",
        "  ax[0,0].hist(v[:,1],color='blue',alpha=0.5,bins=np.linspace(0,1000000000*0.2*0.25*0.2,50),label='1st Particle')  # Blue for Particle\n",
        "  ax[0,0].set_title('Squared pT Distribution of Real samples',fontweight=\"bold\",color='yellow',fontsize=20)\n",
        "  ax[0,0].legend()\n",
        "\n",
        "  ax[0,1].hist(w[:,0],color='red',alpha=0.5,bins=np.linspace(0,1000000000*0.2*0.25*0.2,50),label='Cluster') # Red For Cluster\n",
        "  ax[0,1].hist(w[:,1],color='blue',alpha=0.5,bins=np.linspace(0,1000000000*0.2*0.25*0.2,50),label='1st Particle')  # Blue for Particle\n",
        "  ax[0,1].set_title('Squared pT Distribution of Fake samples',fontweight=\"bold\",color='yellow',fontsize=20)\n",
        "  ax[0,1].legend()\n",
        "\n",
        "  ax[1,0].hist(g[:,0],color='green',bins=np.linspace(0,1000000000*0.2*0.25*0.2,50),alpha=0.5,label='Real') #\n",
        "  ax[1,0].hist(h[:,0],color='purple',bins=np.linspace(0,1000000000*0.2*0.25*0.2,50),alpha=0.5,label='Fake')  # \n",
        "  ax[1,0].set_title('Squared invariant mass of Cluster Distribution',fontweight=\"bold\",color='yellow',fontsize=20)\n",
        "  ax[1,0].legend() \n",
        "  \n",
        " \n",
        "  ax[1,1].axis('off')\n",
        "  \n",
        "  #fig.subplots_adjust(hspace=1000)\n",
        "  fig.tight_layout(pad =5)\n",
        "  #plt.savefig('test.pdf')         #    These two lines to \n",
        "  #files.download('test.pdf')      #    Download data\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMgjeBM33cZH"
      },
      "source": [
        "Following Cell Block calculates and returns the Squared Invariant Mass of Cluster. \\\\\n",
        "Inputs are : \\\\\n",
        " \n",
        "px,py,pz = 3 momenta of cluster \\\\\n",
        "px1, py1, pz1 = 3 momenta of 1st particle \\\\\n",
        "m1,m2 = Invariant masses of particles \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuhwLTXv3b_c"
      },
      "source": [
        "def calcmsqr(px,py,pz,px1,py1,pz1,m1,m2):\n",
        "  t1 = (px1*px1)+(py1*py1)+(pz1*pz1)+(m1*m1) # E1*E1\n",
        "  E1 = np.sqrt(t1)\n",
        "  px2 = px-px1\n",
        "  py2 = py-py1\n",
        "  pz2 = pz-pz1\n",
        "  t2 = (px2*px2)+(py2*py2)+(pz2*pz2)+(m2*m2) # E2*E2\n",
        "  E2 = np.sqrt(t2)\n",
        "  E = E1 + E2\n",
        "  t3 = (E*E)-(px*px)-(py*py)-(pz*pz)\n",
        "  return t3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QViHuc8936Vz"
      },
      "source": [
        "Following Cell Block prints the accuracy of Discriminator model for 100 samples. It also plots the Squared pT Distribution for both real and fake samples. This function will be called later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhquu_y44FZ3"
      },
      "source": [
        "def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\n",
        "  # prepare real samples\n",
        "  x_real, y_real = generate_real_samples(n)\n",
        "  # evaluate discriminator on real examples\n",
        "  _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
        "\t# prepare fake examples\n",
        "  x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
        "  # evaluate discriminator on fake examples\n",
        "  _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
        "  # summarize discriminator performance\n",
        "  print(epoch, acc_real, acc_fake)\n",
        "  display(x_real,x_fake)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVZK2Jb9BBbg"
      },
      "source": [
        "Rest of the GAN Model is almost same as Tutorial : \n",
        "[How to Develop a 1D Generative Adversarial Network From Scratch in Keras\n",
        "](https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-1-dimensional-function-from-scratch-in-keras/) \\\\\n",
        "Only modifications are :\n",
        "Increasesd the number of layers in both Discriminator & Generator model\n",
        "definition in proportion to number of inputs or number of outputs ( sample size ) \\\\\n",
        "for sample size=6 , 6/2=3 , thus all layers are increased three times. \\\\\n",
        "Increased the size of Latent Points Generated by factor of 3000 \\\\\n",
        "latent_dim =15 ( can also be increased in proportion) \\\\\n",
        "n_epochs = 10000 , n_batch = 1024 , n_eval = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFWnrDZA4Ibe"
      },
      "source": [
        "Discriminator Model :\n",
        "*   The define_discriminator() : function below defines and returns the discriminator model. \n",
        "*   It takes 6 inputs here which are our three momentum values of cluster and one particle. \\\\\n",
        "*   It is a sequential model i.e a linear stack of layers.\n",
        "*   model.add() : Generates fully connected layers. It take sample size as 25*3 = 75. \\\\\n",
        "*   Here, we have used ReLU activation function which maps any negative input too. I suggest in next study, we take  [Leaky ReLU actiavtion function](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) which allows a small positive gradient , this may prevents gradient from dying out during trainings which could lead to better outcomes.\n",
        "* It also uses He [weight initialization](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/)and it will use a sigmoid activation function which would give output in (0,1). \n",
        "*The model will minimize the binary cross entropy loss function, and [the Adam version of stochastic gradient descent](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) will be used because it is very effective.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7KszDbN4NGK"
      },
      "source": [
        "# define the standalone discriminator model\n",
        "def define_discriminator(n_inputs=6):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(75, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6UyP9jyWYdR"
      },
      "source": [
        "The following cell prints the summary of discriminator model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d_h-MSlWZBt"
      },
      "source": [
        "# define the discriminator model\n",
        "model = define_discriminator()\n",
        "# summarize the model\n",
        "model.summary()\n",
        "# plot the model\n",
        "plot_model(model, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdImmq0k4dW6"
      },
      "source": [
        "Generator Model : Now, we define the generator model. \\\\\n",
        "\n",
        "The define_generator() function below defines and returns the generator model.\n",
        "\n",
        "*   The define_generator() function : It takes laten_dim as input and it gives six outputs (which are the momentum values of cluster and particle).\n",
        "*   Latent_dim : Laten variable is hidden variable. Laten space is a multi-dimensional vector space of these variables. We define the size of latent space according to our requirement. The size of the latent dimension is parameterized in case we want to play with it later.\n",
        "*   model.add() :\n",
        "Generates a fully connected layer. Here, we can see that it takes a value given as 45, which is sample size, taken as 15*3 (given in tutorial).\n",
        "* Here, we have used ReLU activation function which maps any negative input too. I suggest in next study, we take Leaky ReLU actiavtion function which allows a small positive gradient , this may prevents gradient from dying out during trainings which could lead to better outcomes.\n",
        "* It also uses He weight initialization and it will use a linear activation function. I suggest, after normalization we must try to use sigmoid activation function which would give output in (0,1). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NUKKtEz4hg1"
      },
      "source": [
        "# define the standalone generator model\n",
        "def define_generator(latent_dim, n_outputs=6):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(45, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
        "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h9RyeIXWxTO"
      },
      "source": [
        "The following cell prints the summary of generator model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87TjOROrWxzg"
      },
      "source": [
        "model = define_generator(5)\n",
        "# summarize the model\n",
        "model.summary()\n",
        "# plot the model\n",
        "plot_model(model, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aTgU3r94ozz"
      },
      "source": [
        "Combined generator and discriminator model : (Theory)\n",
        "\n",
        "Now, the weights and bias in the generator are updated based on the performance of the discriminator model. If the discriminator is effectively detecting the fake samples, the generator is updated more, and when the discriminator is giving confused outputs, the generator model is updated less. Thus, the generator receives as input random points in the latent space, generates samples and provide it into the discriminator model directly, which gets classified between real or fake, after this the output of this model is used to update the the generator.  \\\\\n",
        "The discriminator model can be trained in a standalone manner on each example as it is just a classifier. \\\\\n",
        "The generator model is only concerned with the discriminator’s performance on fake examples. It tries to generate such samples with which it can fool the discriminator better. \\\\\n",
        "\n",
        "thus, it can be seen in code below, that  we have marked the discriminator as not trainable when it is part of the GAN model so that, they can not be updated and overtrained on fake examples. \\\\\n",
        "Similarly, when training the generator,  we want the discriminator to think that the samples output by the generator are real, not fake. Thus, when the generator is trained as part of the GAN model, rhe generated samples are marked as real (1). The discriminator will then classify the generated samples as not real (0) or a low probability value (0.3 or 0.5). \\\\\n",
        "The backpropagation process used to update the model weights will see this as a large error and will update the weights and bias of generator model weights to correct for this error, in turn making the generator better.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDAFoE6may89"
      },
      "source": [
        "Combined generator and discriminator model which will update the generator :\n",
        "\n",
        "* Inputs: Point in latent space. \\\\\n",
        "* Outputs: Tell if the sample is real or fake. \\\\\n",
        "* The define_gan() function takes as arguments the already-defined generator and discriminator models and creates the new logical third model subsuming these two models. \\\\\n",
        "* The GAN model then uses the same binary cross entropy loss function as the discriminator and the efficient Adam version of stochastic gradient descent. \\\\\n",
        "Note : The weights in the discriminator are marked as not trainable, which only affects the weights as seen by the GAN model and not the standalone discriminator model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjtfPBgb4paH"
      },
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(generator, discriminator):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\tdiscriminator.trainable = False\n",
        "\t# connect them\n",
        "\tmodel = Sequential()\n",
        "\t# add generator\n",
        "\tmodel.add(generator)\n",
        "\t# add the discriminator\n",
        "\tmodel.add(discriminator)\n",
        "\t# compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N__QgHJS4ucx"
      },
      "source": [
        "Generating fake samples :\n",
        "\n",
        "Now, we will use the generator model to generate the same number of fake samples.\n",
        "\n",
        "the generate_latent_points() function developed in the generator section above : generates the same number of points in the latent space. \n",
        "These will then be passed to the generator model and used in training.\n",
        "\n",
        "The generate_fake_samples() function : generates these fake samples and the associated class label of 0 which will be useful later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntT6BJ4U43Qr"
      },
      "source": [
        "def generate_latent_points(latent_dim, n):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n)*3000 \n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n):\n",
        "  # generate points in latent space\n",
        "  x_input = generate_latent_points(latent_dim, n)\n",
        "  # predict outputs\n",
        "  X = generator.predict(x_input)\n",
        "  # create class labels\n",
        "  y = zeros((n, 1))\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-18Sjai46nx"
      },
      "source": [
        "Training the GAN Model :\n",
        "The Discriminator is trained to assign 0 to the fake\n",
        "data and 1 to real data. \\\\\n",
        "The Generator is trained such that the Discriminator assigns 1 to the fake examples produced by it. \\\\\n",
        "\n",
        "we have to first update the discriminator model with real and fake samples, then update the generator via the composite model. Thus, combining elements from the train_discriminator() function and the train_gan() function. We need to make sure that the generate_fake_samples() function use the generator model to generate fake samples instead of generating random numbers, which we make sure by past g_model, latent_dim and half_batch as arguments in generate_fake_samples() function.\n",
        "\n",
        "Here, we are manually enumerating the training epochs and for each epoch generating a half batch of real examples and a half batch of fake examples, and updating the model on each, e.g. one whole batch of examples. \n",
        "\n",
        "The model can then be evaluated on the generated examples and we can report the classification accuracy on the real and fake samples.\n",
        "\n",
        "The train() function below implements this, training the model 10000 times (n_epochs)  for 1024 samples (n_batch) at an interval of 100 (n_eval).\n",
        "Half batch is trained for real samples and other half for fake.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVu1iFC85E36"
      },
      "source": [
        "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=1024, n_eval=100):\n",
        "\t# determine half the size of one batch, for updating the discriminator\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_epochs):\n",
        "\t\t# prepare real samples\n",
        "\t\tx_real, y_real = generate_real_samples(half_batch)\n",
        "\t\t# prepare fake examples\n",
        "\t\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t# update discriminator\n",
        "\t\td_model.train_on_batch(x_real, y_real)\n",
        "\t\td_model.train_on_batch(x_fake, y_fake)\n",
        "\t\t# prepare points in latent space as input for the generator\n",
        "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t# create inverted labels for the fake samples\n",
        "\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t# update the generator via the discriminator's error\n",
        "\t\tgan_model.train_on_batch(x_gan, y_gan)\n",
        "\t\t# evaluate the model every n_eval epochs\n",
        "\t\tif (i+1) % n_eval == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, d_model, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J9lcIo15HmD"
      },
      "source": [
        "Final model :\n",
        "Latent dimension is made three times (5*3)\n",
        "In the given model we have six inputs, When there are 2 inputs, the latent dimension was taken as 6, so here in our model we have made such changes.\n",
        "Now here, we call back our previous functions and run the training in final line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z25duqyg5OL6"
      },
      "source": [
        "# size of the latent space\n",
        "latent_dim = 15\n",
        "# create the discriminator\n",
        "discriminator = define_discriminator()\n",
        "# create the generator\n",
        "generator = define_generator(latent_dim)\n",
        "# create the gan\n",
        "gan_model = define_gan(generator, discriminator)\n",
        "# train model\n",
        "train(generator, discriminator, gan_model, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVsgKpoq4F8L"
      },
      "source": [
        "This one uses the summarize performance function described above. (n=6000 is the number of iterations, though I am not sure)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFdRFn8G5S-k"
      },
      "source": [
        "summarize_performance(1,generator, discriminator, latent_dim, n=6000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iIQ7ual9dcr"
      },
      "source": [
        "The following code displays the fake samples generated by generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw-kIz5p5Ts_"
      },
      "source": [
        "x,y =generate_fake_samples(generator,latent_dim,5)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vs1bm729yD5"
      },
      "source": [
        "The following code displays the real samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH9ix1M35WJA"
      },
      "source": [
        "x,y =generate_real_samples(5)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySzV4gN9_OVJ"
      },
      "source": [
        "# References :\n",
        "\n",
        "Article : [How to Develop a 1D Generative Adversarial Network From Scratch in Keras](https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-1-dimensional-function-from-scratch-in-keras/)\n",
        "\n",
        "Book name :\n",
        "[GANs in Action, Chapter 3 : Your first GAN: Generating handwritten digits](https://www.manning.com/books/gans-in-action)"
      ]
    }
  ]
}